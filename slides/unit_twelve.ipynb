{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple String Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word: top\n",
      "Cleaned word: t^p\n"
     ]
    }
   ],
   "source": [
    "word= \"top\"\n",
    "new_word= word.translate(str.maketrans(\"o\",\"^\"))# str -> class in Pyton 3\n",
    "print(\"Original word:\", word)\n",
    "print(\"Cleaned word:\", new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word: food\n",
      "Cleaned word: f^^d\n"
     ]
    }
   ],
   "source": [
    "word= \"food\"\n",
    "new_word= word.translate(str.maketrans(\"o\",\"^\"))\n",
    "print(\"Original word:\", word)\n",
    "print(\"Cleaned word:\", new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word: jumping\n",
      "Cleaned word: jumper!\n"
     ]
    }
   ],
   "source": [
    "word= \"jumping\"\n",
    "new_word= word.translate(str.maketrans(\"ing\",\"er!\"))\n",
    "print(\"Original word:\", word)\n",
    "print(\"Cleaned word:\", new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "the first two maketrans arguments must have equal length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-7644f7704ba5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"jumping\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_word\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ing\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"erssss!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Original word:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cleaned word:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: the first two maketrans arguments must have equal length"
     ]
    }
   ],
   "source": [
    "word= \"jumping\"\n",
    "# This will not work\n",
    "new_word= word.translate(str.maketrans(\"ing\",\"erssss!\"))\n",
    "print(\"Original word:\", word)\n",
    "print(\"Cleaned word:\", new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumpersssss!\n"
     ]
    }
   ],
   "source": [
    "new_word=word.replace(\"ing\", \"ersssss!\")\n",
    "print(new_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Handling Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "punc=string.punctuation\n",
    "print(punc) # print(len(punc)) --> 32 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet: Hey, I really enjoyed the food :) #happy!\n",
      "Cleaned tweet: Hey I really enjoyed the food  happy\n"
     ]
    }
   ],
   "source": [
    "tweet=\"Hey, I really enjoyed the food :) #happy!\"\n",
    "punc=string.punctuation\n",
    "cleaned=tweet.translate(str.maketrans(\"\",\"\", punc))\n",
    "print(\"Original tweet:\", tweet)\n",
    "print(\"Cleaned tweet:\",cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "if you give only one argument to maketrans it must be a dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-6014eac496b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Hey, I really enjoyed the food :) #happy!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcleaned\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Original tweet:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cleaned tweet:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcleaned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: if you give only one argument to maketrans it must be a dict"
     ]
    }
   ],
   "source": [
    "tweet=\"Hey, I really enjoyed the food :) #happy!\"\n",
    "punc=string.punctuation\n",
    "# Will throw an error\n",
    "cleaned=tweet.translate(str.maketrans(punc))\n",
    "print(\"Original tweet:\", tweet)\n",
    "print(\"Cleaned tweet:\",cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet: Hey, I really enjoyed the food :) #happy!\n",
      "Cleaned tweet: Hey I really enj^yed the f^^d  happy\n"
     ]
    }
   ],
   "source": [
    "cleaned=tweet.translate(str.maketrans(\"o\",\"^\", punc))\n",
    "print(\"Original tweet:\", tweet)\n",
    "print(\"Cleaned tweet:\",cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffThe Project Gutenberg EBook of The Complete Works of William Shakespeare, by \\n', 'William Shakespeare\\n', '\\n', 'This eBook is for the use of anyone anywhere at no cost and with\\n', 'almost no restrictions whatsoever.  You may copy it, give it away or\\n']\n"
     ]
    }
   ],
   "source": [
    "my_text=open(\"shakespeare.txt\", \"r\").readlines()\n",
    "print(my_text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of 7\n",
      "the 7\n",
      "ebook 6\n",
      "shakespeare 5\n",
      "this 5\n",
      "project 4\n",
      "gutenberg 4\n",
      "william 4\n",
      "complete 3\n",
      "it 3\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "punc=string.punctuation\n",
    "from collections import defaultdict\n",
    "#----------------------------------\n",
    "def get_dict(sentences):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "    input: @sentences: a list of sentences\n",
    "    returns: a dictionary of the words in the sentences.\n",
    "             dict key is a word and value is word frequency\n",
    "    \"\"\"\n",
    "    word_freq=defaultdict(int)\n",
    "    for sent in sentences:\n",
    "        sent= sent.translate(str.maketrans(\"\",\"\", punc))\n",
    "        words=sent.lower().split()\n",
    "        for w in words:\n",
    "            word_freq[w]+=1\n",
    "    return word_freq\n",
    "###############\n",
    "\n",
    "def sort_dict_descendingly(d):\n",
    "    \"\"\"\n",
    "    Sorts by count/value of the \"freqs\" dictionary \n",
    "    in reverse order such that the highest values occur first \n",
    "    \"\"\"\n",
    "    list_tuples =sorted(d.items(), key = lambda x: x[1], reverse=True)\n",
    "    # lt -> [('of', 7), ('the', 7), ('ebook', 6), ('shakespeare', 5)]\n",
    "    return list_tuples\n",
    "\n",
    "#-----------------------------\n",
    "lines=open(\"shakespeare.txt\", \"r\").readlines()\n",
    "sentences=lines[:30]\n",
    "#-----------------------------\n",
    "freqs_dict=get_dict(sentences)\n",
    "lt= sort_dict_descendingly(freqs_dict)\n",
    "#-----------------------------\n",
    "for i in lt:\n",
    "    w=i[0] # word\n",
    "    freq=i[-1] # freq of word\n",
    "    if w and freq > 2:\n",
    "        print(w, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# This will write freqs to a file\n",
    "lines=open(\"shakespeare.txt\", \"r\").readlines()\n",
    "freqs_dict=get_dict(lines)\n",
    "lt= sort_dict_descendingly(freqs_dict)\n",
    "out_file=open(\"./new_word_list.txt\", \"w\")\n",
    "for i in lt:\n",
    "    # For readability\n",
    "    w=i[0]\n",
    "    freq=i[-1]\n",
    "    out_file.write(w+\"\\t\"+str(freq)+\"\\n\")\n",
    "    \n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\t27824\r\n",
      "and\t26791\r\n",
      "i\t20681\r\n",
      "to\t19261\r\n",
      "of\t18289\r\n",
      "a\t14667\r\n",
      "you\t13716\r\n",
      "my\t12481\r\n",
      "that\t11135\r\n",
      "in\t11027\r\n"
     ]
    }
   ],
   "source": [
    "!head -10 new_word_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identical\t1\r\n",
      "digits\t1\r\n",
      "digit\t1\r\n",
      "10234\t1\r\n",
      "httpwwwgutenbergorg102310234\t1\r\n",
      "24689\t1\r\n",
      "httpwwwgutenbergorg246824689\t1\r\n",
      "alternative\t1\r\n",
      "locating\t1\r\n",
      "httpwwwgutenbergorggutindexall\t1\r\n"
     ]
    }
   ],
   "source": [
    "!tail -10 new_word_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamlet\t107\r\n",
      "hamlets\t10\r\n"
     ]
    }
   ],
   "source": [
    "!grep \"hamlet\" new_word_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\t2053\r\n",
      "loves\t286\r\n",
      "lovers\t74\r\n",
      "lovely\t53\r\n",
      "lover\t52\r\n",
      "loved\t47\r\n",
      "lovell\t40\r\n",
      "glove\t36\r\n",
      "beloved\t29\r\n",
      "gloves\t17\r\n"
     ]
    }
   ],
   "source": [
    "!grep \"love\" new_word_list.txt | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king\t2861\r\n",
      "being\t662\r\n",
      "nothing\t633\r\n",
      "bring\t444\r\n",
      "thing\t355\r\n",
      "things\t338\r\n",
      "kings\t284\r\n",
      "buckingham\t254\r\n",
      "something\t180\r\n",
      "bolingbroke\t175\r\n"
     ]
    }
   ],
   "source": [
    "!grep \"ing\" new_word_list.txt | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\t2812\r\n",
      "too\t1232\r\n",
      "look\t828\r\n",
      "blood\t639\r\n",
      "poor\t626\r\n",
      "fool\t461\r\n",
      "bloody\t224\r\n",
      "looks\t214\r\n",
      "foot\t169\r\n",
      "took\t158\r\n"
     ]
    }
   ],
   "source": [
    "!grep \"oo\" new_word_list.txt | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Take a look at the preface here: http://www.nltk.org/book/ch00.html\n",
    "* This tutorial is based on Python 2.7, but it shouldn't be an issue to write the same code for Python 3 as the differences are minimal so long as the tutorial is concerned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('gutenberg')\n",
    "# nltk.download('genesis')\n",
    "# nltk.download('inaugural')\n",
    "# nltk.download('nps_chat')\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the free fellow our power opportunity of that by order on a years more\n",
      "necessary time other experienced duties own\n"
     ]
    }
   ],
   "source": [
    "text4.similar(\"patriotic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man 102\n",
      "woman 3\n",
      "father 4\n",
      "mother 4\n"
     ]
    }
   ],
   "source": [
    "# Counting word frequencies:\n",
    "words=[\"man\", \"woman\", \"father\", \"mother\"]\n",
    "for w in words:\n",
    "    print(w, text4.count(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'happy']\n",
      "['we', \"'re\", 'playing', 'tennis']\n",
      "['we', \"'ll\", 'study']\n",
      "['They', \"'ve\", 'cooked']\n"
     ]
    }
   ],
   "source": [
    "# Word tokenization with NLTK:\n",
    "import nltk\n",
    "raw=[\"I'm happy\", \"we're playing tennis\", \"we'll study\",\\\n",
    "     \"They've cooked\"]\n",
    "for i in raw:\n",
    "    tokens=nltk.word_tokenize(i)\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Let's look at the text4: Inaugural Address Corpus\n",
    "* NLTK can show a word in context, called a concordance (with a given text window size):\n",
    "* **width:** a parameter forthe window size of surrounding character context\n",
    "* **lines:** a parameter for the number of lines returned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 10 of 74 matches:\n",
      "ons may come . The Negroes are now Americans . Their ancestors came here years \n",
      " not . And yet we are not the less Americans on that account . We shall be the \n",
      "em now secure ; and there comes to Americans the profound assurance that our re\n",
      "d me . I am certain that my fellow Americans expect that on my induction into t\n",
      "urricanes of disaster . In this we Americans were discovering no wholly new tru\n",
      "freedom is an ebbing tide . But we Americans know that this is not true . Eight\n",
      "re not content to stand still . As Americans , we go forward , in the service o\n",
      "be simple and its words brief . We Americans of today , together with our allie\n",
      "charge of this responsibility , we Americans know and we observe the difference\n",
      "in of trading honor for security . Americans , indeed all free men , remember t\n"
     ]
    }
   ],
   "source": [
    "text4.concordance(\"Americans\", width=80, lines=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www.gutenberg.org', '**', 'this', 'is', 'a', 'copyrighted', 'project', 'gutenberg', 'ebook', ',', 'details', 'below', '**', '**', 'please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file', '.', '**', 'title', ':', 'the', 'complete', 'works', 'of', 'william', 'shakespeare', 'author', ':', 'william', 'shakespeare', 'posting', 'date', ':', 'september']\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "from nltk import word_tokenize, Text\n",
    "text_string=codecs.open(\"shakespeare.txt\", \"r\", \"utf-8\").read() # Opens for reading and gets you the file content as a list\n",
    "text_string=text_string.lower()\n",
    "tokens = word_tokenize(text_string)\n",
    "print(type(tokens))\n",
    "print(tokens[50:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "['project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www.gutenberg.org', '**', 'this', 'is', 'a', 'copyrighted', 'project', 'gutenberg', 'ebook', ',', 'details', 'below', '**', '**', 'please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file', '.', '**', 'title', ':', 'the', 'complete', 'works', 'of', 'william', 'shakespeare', 'author', ':', 'william', 'shakespeare', 'posting', 'date', ':', 'september']\n"
     ]
    }
   ],
   "source": [
    "text = Text(tokens)\n",
    "print(\"*\"*50)\n",
    "print(text[50:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hamlet: Entire Play\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Tragedy of Hamlet, Prince of Denmark\n",
      "\n",
      "Shakespeare homepage \n",
      "    | Hamlet \n",
      "    | Entire play\n",
      "\n",
      "ACT I\n",
      "SCENE I. Elsinore. A platform before the castle.\n",
      "\n",
      "FRANCISCO at his post. Enter to him BERNARDO\n",
      "\n",
      "BERNARDO\n",
      "\n",
      "Who's there?\n",
      "\n",
      "FRANCISCO\n",
      "\n",
      "Nay, answer me: stand, and unfold you\n"
     ]
    }
   ],
   "source": [
    "# Fetching and cleaning a webpage:\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "url=\"http://shakespeare.mit.edu/hamlet/full.html\"\n",
    "page = urlopen(url)\n",
    "soup = BeautifulSoup(page.read())   \n",
    "raw = BeautifulSoup.get_text(soup)  \n",
    "print(raw[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hamlet', ':', 'entire', 'play', 'the', 'tragedy', 'of', 'hamlet', ',', 'prince', 'of', 'denmark', 'shakespeare', 'homepage', '|', 'hamlet', '|', 'entire', 'play', 'act']\n"
     ]
    }
   ],
   "source": [
    "raw=raw.lower()\n",
    "tokens=nltk.word_tokenize(raw)\n",
    "print(tokens[:20])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
